import sys
import os
import shutil
import gc
from operator import itemgetter
import time  
# -----------------------------------------------------------------------------
# 0. é…ç½®å›½å†…é•œåƒ & Linux å…¼å®¹è¡¥ä¸ (æœ€ä¼˜å…ˆæ‰§è¡Œ)
# -----------------------------------------------------------------------------
# å¼ºåˆ¶ä½¿ç”¨ Hugging Face å›½å†…é•œåƒ (è§£å†³ Network unreachable é—®é¢˜)
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# Linux ChromaDB è¡¥ä¸
try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

# -----------------------------------------------------------------------------
# 1. Imports
# -----------------------------------------------------------------------------
import streamlit as st
from dotenv import load_dotenv

# Embeddings
from langchain_huggingface import HuggingFaceEmbeddings

# LLM
from langchain_openai import ChatOpenAI

# Vector Database
from langchain_community.vectorstores import Chroma

# Document Processing
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Core Primitives (LCEL)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableBranch

# -----------------------------------------------------------------------------
# 2. åˆå§‹åŒ–é…ç½®
# -----------------------------------------------------------------------------
load_dotenv() # åŠ è½½ .env æ–‡ä»¶

DATA_PATH = "./data"
DB_PATH = "./db"

os.makedirs(DATA_PATH, exist_ok=True)
st.set_page_config(page_title="æœ¬åœ° RAG çŸ¥è¯†åº“ (DeepSeek Coreç‰ˆ)", layout="wide")

# -----------------------------------------------------------------------------
# 3. æ ¸å¿ƒé€»è¾‘ï¼šå‘é‡æ•°æ®åº“æ„å»º (å·²ä¿®å¤æ–‡ä»¶é” Bug)
# -----------------------------------------------------------------------------
@st.cache_resource(show_spinner=True)
def rebuild_vector_db():
    # 1. å¼ºåˆ¶æ¸…ç†æ—§èµ„æº
    # å°è¯•è®¿é—®å¹¶æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§å˜é‡ï¼Œå¼ºåˆ¶æ–­å¼€æ•°æ®åº“è¿æ¥
    if 'vectorstore' in globals():
        del globals()['vectorstore']
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜ä¸­çš„æ–‡ä»¶å¥æŸ„
    gc.collect()
    
    # ğŸ›‘ å…³é”®ä¿®å¤ï¼šæš‚åœ 1 ç§’ï¼Œç­‰å¾…æ“ä½œç³»ç»Ÿå®Œå…¨é‡Šæ”¾ SQLite æ–‡ä»¶é”
    # Linux ä¸Šçš„æ–‡ä»¶åˆ é™¤æœ‰æ—¶æ˜¯å¼‚æ­¥çš„ï¼Œä¸ç­‰å¾…ä¼šå¯¼è‡´ "ReadOnly" æˆ– "Locked" é”™è¯¯
    time.sleep(1)

    # 2. æ¸…ç†æ—§æ•°æ®åº“æ–‡ä»¶å¤¹
    if os.path.exists(DB_PATH):
        try:
            shutil.rmtree(DB_PATH)
            # å†æ¬¡ç­‰å¾…æ–‡ä»¶ç³»ç»ŸåŒæ­¥
            time.sleep(0.5)
        except Exception as e:
            st.error(f"æ¸…ç†æ—§æ•°æ®åº“å¤±è´¥ (æ–‡ä»¶å¯èƒ½ä»è¢«å ç”¨ï¼Œè¯·é‡å¯æœåŠ¡): {e}")
            return None

    # 3. æ‰«ææ•°æ®ç›®å½•
    documents = []
    if not os.path.exists(DATA_PATH):
        os.makedirs(DATA_PATH)
        
    categories = [d for d in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, d))]
    
    if not categories:
        return None

    status_text = st.empty()
    status_text.info("æ­£åœ¨æ‰«ææ–‡æ¡£å¹¶é‡å»ºçŸ¥è¯†åº“...")

    for category in categories:
        cat_path = os.path.join(DATA_PATH, category)
        files = [f for f in os.listdir(cat_path) if f.lower().endswith(".pdf")]
        
        for file in files:
            file_path = os.path.join(cat_path, file)
            try:
                loader = PyPDFLoader(file_path)
                docs = loader.load()
                for doc in docs:
                    doc.metadata["category"] = category
                    doc.metadata["source"] = file
                documents.extend(docs)
            except Exception as e:
                st.warning(f"æ— æ³•åŠ è½½æ–‡ä»¶ {file}: {e}")

    if not documents:
        status_text.warning("æœªæ‰¾åˆ°ä»»ä½• PDF æ–‡æ¡£ã€‚")
        return None

    # 4. æ–‡æœ¬åˆ‡åˆ†
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(documents)

    # 5. é‡æ–°åˆå§‹åŒ– Embeddings å’Œ æ•°æ®åº“
    # ä½¿ç”¨ Hugging Face å›½å†…é•œåƒä¸‹è½½æ¨¡å‹ (å¦‚æœè¿˜æ²¡ä¸‹è½½è¿‡)
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

    try:
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings
            #persist_directory=DB_PATH
        )
    except Exception as e:
        st.error(f"åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
        # å¦‚æœæŠ¥é”™ï¼Œå°è¯•å†æ¸…ç†ä¸€æ¬¡ä»¥ä¾¿ä¸‹æ¬¡é‡è¯•
        shutil.rmtree(DB_PATH, ignore_errors=True)
        return None
    
    status_text.success(f"é‡å»ºå®Œæˆï¼å…± {len(splits)} ä¸ªåˆ‡ç‰‡ã€‚")
    return vectorstore

vectorstore = rebuild_vector_db()

# -----------------------------------------------------------------------------
# 4. ä¾§è¾¹æ ï¼šæ•°æ®ç®¡ç†
# -----------------------------------------------------------------------------
with st.sidebar:
    st.header("ğŸ—‚ï¸ çŸ¥è¯†åº“ç®¡ç†")
    current_categories = [d for d in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, d))]
    
    new_cat = st.text_input("æ–°å»ºåˆ†ç±»æ–‡ä»¶å¤¹", placeholder="ä¾‹å¦‚ï¼šLab_Protocols")
    if st.button("åˆ›å»ºåˆ†ç±»"):
        if new_cat:
            target_dir = os.path.join(DATA_PATH, new_cat)
            if not os.path.exists(target_dir):
                os.makedirs(target_dir)
                st.success(f"å·²åˆ›å»º: {new_cat}")
                st.rerun()

    st.markdown("---")
    selected_cat_upload = st.selectbox("é€‰æ‹©ä¸Šä¼ åˆ†ç±»", ["(è¯·é€‰æ‹©)"] + current_categories)
    uploaded_files = st.file_uploader("ä¸Šä¼  PDF", type=["pdf"], accept_multiple_files=True)
    
    if st.button("ğŸ’¾ ä¿å­˜å¹¶æ›´æ–°"):
        if selected_cat_upload != "(è¯·é€‰æ‹©)" and uploaded_files:
            save_dir = os.path.join(DATA_PATH, selected_cat_upload)
            for uploaded_file in uploaded_files:
                with open(os.path.join(save_dir, uploaded_file.name), "wb") as f:
                    f.write(uploaded_file.getbuffer())
            st.cache_resource.clear()
            st.rerun()

    if st.button("ğŸ”„ å¼ºåˆ¶åˆ·æ–°"):
        st.cache_resource.clear()
        st.rerun()

# -----------------------------------------------------------------------------
# 5. ä¸»ç•Œé¢ï¼šLCEL RAG é€»è¾‘
# -----------------------------------------------------------------------------
st.title("ğŸ§ª å®éªŒå®¤åŠ©æ‰‹AI (DeepSeekç‰ˆ BY å­™åšè¶…)")

search_category = st.selectbox("ğŸ” æœç´¢èŒƒå›´", ["å…¨éƒ¨"] + current_categories, index=0)

if not vectorstore:
    st.info("ğŸ‘ˆ è¯·å…ˆä¸Šä¼ æ–‡æ¡£åˆå§‹åŒ–çŸ¥è¯†åº“ã€‚")
    st.stop()

# --- LLM ---
llm = ChatOpenAI(
    model="deepseek-chat",
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    openai_api_base="https://api.deepseek.com",
    temperature=0.1
)

# --- Retriever ---
search_kwargs = {"k": 4}
if search_category != "å…¨éƒ¨":
    search_kwargs["filter"] = {"category": search_category}
retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)

# --- è¾…åŠ©å‡½æ•° ---
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# --- Step 1: å†å²å›æº¯ (History Aware) ---
contextualize_q_system_prompt = (
    "ç»™å®šèŠå¤©å†å²å’Œç”¨æˆ·é—®é¢˜ï¼Œè¯·å°†é—®é¢˜é‡å†™ä¸ºä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ï¼Œ"
    "ä½¿å…¶æ— éœ€ä¸Šä¸‹æ–‡å³å¯ç†è§£ã€‚ç›´æ¥è¾“å‡ºé‡å†™åçš„é—®é¢˜ï¼Œä¸è¦è§£é‡Šã€‚"
)
contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

history_aware_chain = contextualize_q_prompt | llm | StrOutputParser()

# --- ä¿®æ­£åçš„ RunnableBranch ---
# é€»è¾‘ï¼šå¦‚æœæœ‰ chat_historyï¼Œèµ° history_aware_chainï¼›å¦åˆ™ç›´æ¥é€ä¼  input
retrieval_chain = RunnableBranch(
    (lambda x: len(x.get("chat_history", [])) > 0, history_aware_chain | retriever),
    itemgetter("input") | retriever # <--- ä¿®æ­£ç‚¹ï¼šé»˜è®¤åˆ†æ”¯ç›´æ¥å†™ Runnableï¼Œä¸è¦åŠ å…ƒç»„
)

# --- Step 2: é—®ç­”ç”Ÿæˆ (Stuff Documents) ---
qa_system_prompt = (
    "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å®éªŒå®¤åŠ©æ‰‹ã€‚è¯·ä»…æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡(Context)å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
    "å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´æ˜ä¸çŸ¥é“ã€‚\n\n"
    "Context:\n{context}"
)
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", qa_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

# å®Œæ•´çš„ RAG é“¾
rag_chain = (
    {
        "context": retrieval_chain,
        "input": itemgetter("input"),
        "chat_history": itemgetter("chat_history")
    }
    | RunnablePassthrough.assign(context=lambda x: format_docs(x["context"]))
    | qa_prompt
    | llm
    | StrOutputParser()
)

# ç”¨äºå¼•ç”¨æ˜¾ç¤ºçš„å•ç‹¬é“¾
source_retrieval_chain = retrieval_chain

# -----------------------------------------------------------------------------
# 6. UI äº¤äº’
# -----------------------------------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # æ„å»ºå†å²
        chat_history = []
        for msg in st.session_state.messages[:-1]:
            if msg["role"] == "user":
                chat_history.append(HumanMessage(content=msg["content"]))
            else:
                chat_history.append(AIMessage(content=msg["content"]))

        try:
            # 1. ç”Ÿæˆå›ç­”
            full_response = rag_chain.invoke({
                "input": prompt,
                "chat_history": chat_history
            })
            message_placeholder.markdown(full_response)

            # 2. æ˜¾ç¤ºå¼•ç”¨
            retrieved_docs = source_retrieval_chain.invoke({
                "input": prompt,
                "chat_history": chat_history
            })

            if retrieved_docs:
                with st.expander("ğŸ“š å‚è€ƒæ¥æº"):
                    seen = set()
                    for doc in retrieved_docs:
                        sid = f"{doc.metadata.get('category')} - {doc.metadata.get('source')}"
                        if sid not in seen:
                            st.markdown(f"- `{sid}`")
                            seen.add(sid)

            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {e}")